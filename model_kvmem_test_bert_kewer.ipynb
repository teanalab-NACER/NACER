{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "758e2451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import utils\n",
    "from model_kvmem import ModelKvmem\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22aca6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    print('Epoch:', checkpoint['epoch'])\n",
    "    model_args = checkpoint['args']\n",
    "    print('Model args:', model_args)\n",
    "    model = ModelKvmem(qemb=model_args.qemb, num_hops=model_args.hops)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    kvmem_triples = utils.load_kvmem_triples(model_args.baseline)\n",
    "    return model, kvmem_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6685b3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(modelpath='models/model-kvmem-3-3hops-bert-kewer.pt', qemb='kewer', resultpath='results/kvmem-3-3hops-bert-kewer-test.txt', testsplit='test')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--modelpath', default='models/model-kvmem-3-3hops-bert-kewer.pt', help=\"Path to the trained model.\")\n",
    "parser.add_argument('--testsplit', default='test', choices=['train', 'dev', 'test'],\n",
    "                    help=\"QBLink split used for testing\")\n",
    "parser.add_argument('--qemb', default='kewer', choices=['kewer', 'blstatic', 'bldynamic', 'bert'],\n",
    "                            help=\"How to embed question text. \"\n",
    "                                \"kewer: mean of KEWER embeddings of tokens and linked entities, \"\n",
    "                                \"bldynamic: Bi-LSTM embedding trained as part of the model, \"\n",
    "                                \"blstatic: Static pre-trained Bi-LSTM embedding\")\n",
    "parser.add_argument('--resultpath', default='results/kvmem-3-3hops-bert-kewer-test.txt', help=\"Path to save the per-query metric values\")\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0290a523",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = utils.load_qblink_split(args.testsplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a23a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "kewer = utils.load_kewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6adceb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_probs = utils.load_word_probs()\n",
    "question_entities = utils.load_question_entities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc7fa8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49\n",
      "Model args: Namespace(baseline='baseline-3', epochs=50, gpu=0, hops=3, loadmodel=None, qemb='kewer', savemodel='models/model-kvmem-3-3hops-bert-kewer.pt')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model, kvmem_triples = load_model(args.modelpath)\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3517ee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19751763",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_10 = 0\n",
    "correct_1 = 0\n",
    "mrr = 0.0\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8777136",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.resultpath:\n",
    "    resultfile = open(args.resultpath, \"w\")\n",
    "    resultfile.write('question_id\\tcorrect_1\\tcorrect_10\\tmrr\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34102d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gp8020/miniconda3/envs/py3.7/lib/python3.7/site-packages/ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "for sequence in test_split:\n",
    "    for question in ['q1', 'q2', 'q3']:\n",
    "        question_id = str(sequence[question]['t_id'])\n",
    "        target_entity = f\"<http://dbpedia.org/resource/{sequence[question]['wiki_page']}>\"\n",
    "        if question_id in kvmem_triples:\n",
    "            question_text = sequence[question]['quetsion_text']\n",
    "            key_embeddings = []\n",
    "            value_embeddings = []\n",
    "            value_entities = set()\n",
    "\n",
    "            for subj, pred, obj in kvmem_triples[question_id]:\n",
    "                if subj in kewer.wv and pred in kewer.wv and obj in kewer.wv:\n",
    "                    key_embedding = kewer.wv[subj] + kewer.wv[pred]\n",
    "                    key_embedding = key_embedding / np.linalg.norm(key_embedding)\n",
    "                    key_embeddings.append(key_embedding)\n",
    "                    value_embedding = kewer.wv[obj]\n",
    "                    value_embeddings.append(value_embedding)\n",
    "                    value_entities.add(obj)\n",
    "\n",
    "            candidate_entities = list(value_entities)\n",
    "            candidate_embeddings = []\n",
    "            for candidate_entity in candidate_entities:\n",
    "                candidate_embedding = kewer.wv[candidate_entity]\n",
    "                candidate_embedding = candidate_embedding / np.linalg.norm(candidate_embedding)\n",
    "                candidate_embeddings.append(candidate_embedding)\n",
    "\n",
    "            if target_entity not in value_entities:\n",
    "                continue\n",
    "                \n",
    "            input_ids, kewer_question_embedding = utils.embed_question_bert(question_text, kewer.wv, \n",
    "                                                                            word_probs, tokenizer)\n",
    "            input_ids_tensor = torch.as_tensor(input_ids)\n",
    "            bert_emd = model.BERT.bert(input_ids_tensor.unsqueeze(dim = 0))['last_hidden_state']\n",
    "            bert_question_embedding = torch.sum(bert_emd.squeeze(), dim = 0)\n",
    "            bert_question_embedding = bert_question_embedding / torch.norm(bert_question_embedding)\n",
    "     \n",
    "            with torch.no_grad():\n",
    "                scores = model(torch.tensor(kewer_question_embedding, dtype=torch.float32),\n",
    "                               torch.tensor(bert_question_embedding, dtype=torch.float32),\n",
    "                               torch.tensor(key_embeddings, dtype=torch.float32),\n",
    "                               torch.tensor(value_embeddings, dtype=torch.float32),\n",
    "                               torch.tensor(candidate_embeddings, dtype=torch.float32))\n",
    "                # print(scores)\n",
    "\n",
    "            entity_scores = []\n",
    "            for i, entity in enumerate(candidate_entities):\n",
    "                entity_scores.append((entity, scores[i].item()))\n",
    "\n",
    "            ranked_entities = sorted(entity_scores, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "            if args.resultpath:\n",
    "                resultfile.write(question_id + '\\t')\n",
    "            if target_entity == ranked_entities[0][0]:\n",
    "                correct_1 += 1\n",
    "                if args.resultpath:\n",
    "                    resultfile.write('1\\t')\n",
    "            elif args.resultpath:\n",
    "                    resultfile.write('0\\t')\n",
    "            if target_entity in [entity for entity, score in ranked_entities[:10]]:\n",
    "                correct_10 += 1\n",
    "                if args.resultpath:\n",
    "                    resultfile.write('1\\t')\n",
    "            elif args.resultpath:\n",
    "                    resultfile.write('0\\t')\n",
    "            found_target_entity = False\n",
    "            for i, (entity, score) in enumerate(ranked_entities):\n",
    "                if entity == target_entity:\n",
    "                    mrr += 1 / (i + 1)\n",
    "                    if args.resultpath:\n",
    "                        resultfile.write(f'{1 / (i + 1)}\\n')\n",
    "                    found_target_entity = True\n",
    "                    break\n",
    "            assert found_target_entity\n",
    "            total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2563a5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits@1: 980\n",
      "Recall@1: 0.6331\n",
      "Hits@10: 1417\n",
      "Recall@10: 0.9154\n",
      "MRR: 0.7288\n",
      "Total: 1548\n"
     ]
    }
   ],
   "source": [
    "print(f\"Hits@1: {correct_1}\")\n",
    "print(f\"Recall@1: {correct_1 / total:.4f}\")\n",
    "print(f\"Hits@10: {correct_10}\")\n",
    "print(f\"Recall@10: {correct_10 / total:.4f}\")\n",
    "print(f\"MRR: {mrr / total:.4f}\")\n",
    "print(f\"Total: {total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
