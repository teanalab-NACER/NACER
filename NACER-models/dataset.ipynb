{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "741fcbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5244546a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(baseline='baseline-4', epochs=100, features_mask=None, gpu=0, hidden='20,10', interaction='mult', loadmodel=None, qemb='kewer', same_w=True, savemodel='models/model-mult-same.pt')\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu', type=int, default=0, help=\"GPU device ID. Use -1 for CPU training\")\n",
    "parser.add_argument('--epochs', type=int, default=100, help=\"Number of training epochs\")\n",
    "parser.add_argument('--hidden', type=str, default='20,10', help=\"Sized of hidden layers, comma-separated\")\n",
    "utils.add_bool_arg(parser, 'same-w', True)  # use the same matrix W for all features\n",
    "parser.add_argument('--interaction', default='mult', choices=['mult', 'add', 'dot'],\n",
    "                    help=\"Interaction function to use\")\n",
    "parser.add_argument('--qemb', default='kewer', choices=['kewer', 'blstatic', 'bldynamic'],\n",
    "                    help=\"How to embed question text. \"\n",
    "                         \"kewer: mean of KEWER embeddings of tokens and linked entities, \"\n",
    "                         \"bldynamic: Bi-LSTM embedding trained as part of the model, \"\n",
    "                         \"blstatic: Static pre-trained Bi-LSTM embedding\")\n",
    "parser.add_argument('--features-mask', nargs='+', type=int, help=\"Features mask for feature ablation study\")\n",
    "parser.add_argument('--savemodel', default='models/model-mult-same.pt', help=\"Path to save the model\")\n",
    "parser.add_argument('--loadmodel', help='Load this model checkpoint before training')\n",
    "parser.add_argument('--baseline', default='baseline-4', help=\"Baseline method triples\")\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d31556d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_inputs = utils.load_feature_inputs()\n",
    "kewer = utils.load_kewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f47da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_probs = None\n",
    "question_entities = None\n",
    "train_question_embeddings = None\n",
    "# dev_question_embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e0d88a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_probs = utils.load_word_probs()\n",
    "question_entities = utils.load_question_entities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6c6afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_question_set(args, qblink_split, overlap_features, feature_inputs, question_embeddings, kewer, word_probs,\n",
    "                      question_entities):\n",
    "    question_set = []\n",
    "    for sequence in qblink_split:\n",
    "        for question in ['q1', 'q2', 'q3']:\n",
    "            question_id = str(sequence[question]['t_id'])\n",
    "            question_text = sequence[question]['quetsion_text']\n",
    "            target_entity = f\"<http://dbpedia.org/resource/{sequence[question]['wiki_page']}>\"\n",
    "            if question_id in overlap_features:\n",
    "\n",
    "                if question == 'q1':\n",
    "                    previous_answer = None\n",
    "                elif question == 'q2':\n",
    "                    previous_answer = f\"<http://dbpedia.org/resource/{sequence['q1']['wiki_page']}>\"\n",
    "                elif question == 'q3':\n",
    "                    previous_answer = f\"<http://dbpedia.org/resource/{sequence['q2']['wiki_page']}>\"\n",
    "                if previous_answer is not None and previous_answer in kewer.wv:\n",
    "                    previous_answer_embedding = kewer.wv[previous_answer].copy()\n",
    "                else:\n",
    "                    previous_answer_embedding = np.zeros(kewer.wv.vector_size, dtype=np.float32)\n",
    "\n",
    "                overlap_feature_array = []\n",
    "                feature_input_arrays = {\n",
    "                    'p': [],\n",
    "                    'lit': [],\n",
    "                    'cat': [],\n",
    "                    'ent': [],\n",
    "                    's': []\n",
    "                }\n",
    "                for i, (entity, entity_overlap_features) in enumerate(overlap_features[question_id].items()):\n",
    "                    assert entity in feature_inputs\n",
    "                    overlap_feature_array.append(entity_overlap_features)\n",
    "                    for feature_type in ['p', 'lit', 'cat', 'ent']:\n",
    "                        if feature_inputs[entity]['counts'][feature_type] > 0:\n",
    "                            feature_input_arrays[feature_type].append(\n",
    "                                feature_inputs[entity]['feature_inputs'][feature_type] /\n",
    "                                feature_inputs[entity]['counts'][feature_type])\n",
    "                        else:\n",
    "                            assert (feature_inputs[entity]['feature_inputs'][feature_type] == 0).all()\n",
    "                            feature_input_arrays[feature_type].append(\n",
    "                                feature_inputs[entity]['feature_inputs'][feature_type])\n",
    "                    feature_input_arrays['s'].append((feature_inputs[entity]['feature_inputs']['lit'] +\n",
    "                                                      feature_inputs[entity]['feature_inputs']['cat'] +\n",
    "                                                      feature_inputs[entity]['feature_inputs']['ent']) / (\n",
    "                                                             feature_inputs[entity]['counts']['lit'] +\n",
    "                                                             feature_inputs[entity]['counts']['cat'] +\n",
    "                                                             feature_inputs[entity]['counts']['ent']))\n",
    "                    if entity == target_entity:\n",
    "                        target_index = i\n",
    "\n",
    "                question_set_item = {\n",
    "                    'overlap_features': np.array(overlap_feature_array, dtype=np.float32),\n",
    "                    'p_inputs': np.array(feature_input_arrays['p'], dtype=np.float32),\n",
    "                    'lit_inputs': np.array(feature_input_arrays['lit'], dtype=np.float32),\n",
    "                    'cat_inputs': np.array(feature_input_arrays['cat'], dtype=np.float32),\n",
    "                    'ent_inputs': np.array(feature_input_arrays['ent'], dtype=np.float32),\n",
    "                    's_inputs': np.array(feature_input_arrays['s'], dtype=np.float32),\n",
    "                    'previous_answer_embedding': previous_answer_embedding,\n",
    "                    'target_index': target_index\n",
    "                }\n",
    "                if args.qemb == 'kewer':\n",
    "                    question_set_item['question_embedding'] = utils.embed_question(question_text, kewer.wv, word_probs,\n",
    "                                                                                   question_entities[question_id])\n",
    "                elif args.qemb == 'blstatic':\n",
    "                    question_set_item['question_embedding'] = utils.get_question_embedding(question_embeddings,\n",
    "                                                                                           int(question_id))\n",
    "                elif args.qemb == 'bldynamic':\n",
    "                    question_set_item['question'] = question_text\n",
    "\n",
    "                question_set.append(question_set_item)\n",
    "    return question_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d7d303c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev examples: 1111\n"
     ]
    }
   ],
   "source": [
    "dev_split = utils.load_qblink_split('dev')\n",
    "dev_overlap_features = utils.load_overlap_features('dev')\n",
    "dev_set = load_question_set(args, dev_split, dev_overlap_features, feature_inputs, dev_question_embeddings, kewer,\n",
    "                            word_probs, question_entities)\n",
    "print('Dev examples:', len(dev_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32b1eab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1111"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_overlap_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75e0862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_question_set(qblink_split, kvmem_triples, kewer, word_probs, question_entities):\n",
    "    question_set = []\n",
    "    for sequence in qblink_split:\n",
    "        for question in ['q1', 'q2', 'q3']:\n",
    "            question_id = str(sequence[question]['t_id'])\n",
    "            target_entity = f\"<http://dbpedia.org/resource/{sequence[question]['wiki_page']}>\"\n",
    "            if question_id in kvmem_triples:\n",
    "                key_embeddings = []\n",
    "                value_embeddings = []\n",
    "                value_entities = set()\n",
    "\n",
    "                for subj, pred, obj in kvmem_triples[question_id]:\n",
    "                    if subj in kewer.wv and pred in kewer.wv and obj in kewer.wv:\n",
    "                        key_embedding = kewer.wv[subj] + kewer.wv[pred]\n",
    "                        key_embedding = key_embedding / np.linalg.norm(key_embedding)\n",
    "                        key_embeddings.append(key_embedding)\n",
    "                        value_embedding = kewer.wv[obj]\n",
    "                        value_embeddings.append(value_embedding)\n",
    "                        value_entities.add(obj)\n",
    "\n",
    "                candidate_embeddings = []\n",
    "                target_index = None\n",
    "                i = 0\n",
    "                \n",
    "                for value_entity in value_entities:\n",
    "                    candidate_embedding = kewer.wv[value_entity]\n",
    "                    candidate_embedding = candidate_embedding / np.linalg.norm(candidate_embedding)\n",
    "                    candidate_embeddings.append(candidate_embedding)\n",
    "                    if value_entity == target_entity:\n",
    "                        target_index = i\n",
    "                    i += 1\n",
    "\n",
    "                if target_index is not None:\n",
    "                    question_text = sequence[question]['quetsion_text']\n",
    "                    question_embedding = utils.embed_question(question_text, kewer.wv, word_probs,\n",
    "                                                              question_entities[question_id])\n",
    "                    question_set.append({\n",
    "                        'question_embedding': np.array(question_embedding, dtype=np.float32),\n",
    "                        'key_embeddings': np.array(key_embeddings, dtype=np.float32),\n",
    "                        'value_embeddings': np.array(value_embeddings, dtype=np.float32),\n",
    "                        'candidate_embeddings': np.array(candidate_embeddings, dtype=np.float32),\n",
    "                        'target_index': target_indexzzz\n",
    "                    })\n",
    "    return question_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7b3aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "kvmem_triples = utils.load_kvmem_triples(args.baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c2e5003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev examples: 839\n"
     ]
    }
   ],
   "source": [
    "dev_set = load_question_set(dev_split, kvmem_triples, kewer, word_probs, question_entities)\n",
    "print('Dev examples:', len(dev_set))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
